{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('AmesHousing.tsv', delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57088.25161263909"
      ]
     },
     "execution_count": 441,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bare bones feature engineering & training\n",
    "def transform_features(df):\n",
    "    return df.copy()\n",
    "\n",
    "def select_features(df):\n",
    "    return df[[\"Gr Liv Area\", \"SalePrice\"]]\n",
    "\n",
    "def train_and_test(df):\n",
    "    num_data = df.select_dtypes(include=['integer', 'float'])\n",
    "    \n",
    "    train = num_data.iloc[:1460, :]\n",
    "    test = num_data.iloc[1460:, :]\n",
    "    \n",
    "    target_col = 'SalePrice'\n",
    "    feature_cols = train.columns.drop(target_col)\n",
    "    \n",
    "    lr = LinearRegression()\n",
    "    lr.fit(train[feature_cols], train[target_col])\n",
    "    predictions = lr.predict(test[feature_cols])\n",
    "    \n",
    "    mse = mean_squared_error(test[target_col], predictions)\n",
    "    rmse = np.sqrt(mse)\n",
    "    \n",
    "    return rmse\n",
    "\n",
    "clean_df = transform_features(data)\n",
    "filtered_df = select_features(clean_df)\n",
    "rmse = train_and_test(filtered_df)\n",
    "\n",
    "rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* for numerical cols with **less than 5% missing**: **impute** the column's **mode**\n",
    "* drop any other columns with missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imputed  0.0  on null values for column: Mas Vnr Area\n",
      "imputed  0.0  on null values for column: BsmtFin SF 1\n",
      "imputed  0.0  on null values for column: BsmtFin SF 2\n",
      "imputed  0.0  on null values for column: Bsmt Unf SF\n",
      "imputed  0.0  on null values for column: Total Bsmt SF\n",
      "imputed  0.0  on null values for column: Bsmt Full Bath\n",
      "imputed  0.0  on null values for column: Bsmt Half Bath\n",
      "imputed  2.0  on null values for column: Garage Cars\n",
      "imputed  0.0  on null values for column: Garage Area\n"
     ]
    }
   ],
   "source": [
    "# for numerical columns with less than 5% missing\n",
    "# impute the column's mode\n",
    "num_data = data.select_dtypes(include=['integer', 'float'])\n",
    "impute_thresh = len(num_data)//20\n",
    "missing_cnt = num_data.isna().sum()\n",
    "\n",
    "to_impute = missing_cnt[(missing_cnt < impute_thresh) & (missing_cnt > 0)]\n",
    "to_impute = list(to_impute.index)\n",
    "                        \n",
    "modes = num_data.mode()\n",
    "for c in to_impute:\n",
    "    mode = modes.loc[0, c]\n",
    "    print('imputed ',mode, ' on null values for column:', c)\n",
    "    data[c] = data[c].fillna(mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cols dropped:  ['Lot Frontage', 'Alley', 'Mas Vnr Type', 'Bsmt Qual', 'Bsmt Cond', 'Bsmt Exposure', 'BsmtFin Type 1', 'BsmtFin Type 2', 'Electrical', 'Fireplace Qu', 'Garage Type', 'Garage Yr Blt', 'Garage Finish', 'Garage Qual', 'Garage Cond', 'Pool QC', 'Fence', 'Misc Feature']\n"
     ]
    }
   ],
   "source": [
    "# drop any other columns with missing values\n",
    "missing_cnt = data.isna().sum()\n",
    "to_drop = list(missing_cnt[missing_cnt > 0].index)\n",
    "data = data.drop(columns = to_drop)\n",
    "print('cols dropped: ', to_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create new features from unusable data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2180   -1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 444,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "years_sold = data['Yr Sold'] - data['Year Built']\n",
    "years_sold[years_sold < 0]        # find bad data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1702   -1\n",
       "2180   -2\n",
       "2181   -1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 445,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "years_since_remod = data['Yr Sold'] - data['Year Remod/Add']\n",
    "years_since_remod[years_since_remod < 0]        # find bad data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Drop rows with negative values for both of these new features\n",
    "data = data.drop(index=[1702, 2180, 2181])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add new feature columns\n",
    "data['Years Before Sale'] = years_sold\n",
    "data['Years Since Remod'] = years_since_remod\n",
    "\n",
    "# drop uneeded original year columns\n",
    "data = data.drop(columns = [\"Year Built\", \"Year Remod/Add\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drop columns that:\n",
    "* that aren't useful for ML\n",
    "* leak data about the final sale\n",
    "\n",
    "read more about columns in the [column description file](https://s3.amazonaws.com/dq-content/307/data_description.txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Drop not useful columns\n",
    "data = data.drop(columns = [\"PID\", \"Order\"])\n",
    "\n",
    "# Drop columns that leak info about the sale\n",
    "data = data.drop(columns = [\"Mo Sold\", \"Sale Condition\", \"Sale Type\", \"Yr Sold\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def transform_features(data):\n",
    "    ## clean missing data,  remove/impute\n",
    "    \n",
    "    # for numerical columns\n",
    "    # for columns with less than 5% missing\n",
    "    # impute the column's mode\n",
    "    num_data = data.select_dtypes(include=['integer', 'float'])\n",
    "    impute_thresh = len(num_data)//20\n",
    "    missing_cnt = num_data.isna().sum()\n",
    "\n",
    "    to_impute = missing_cnt[(missing_cnt < impute_thresh) & (missing_cnt > 0)]\n",
    "    to_impute = list(to_impute.index)\n",
    "\n",
    "    modes = num_data.mode()\n",
    "    for c in to_impute:\n",
    "        mode = modes.loc[0, c]\n",
    "        # print('imputed ',mode, ' on null values for column:', c)\n",
    "        data[c] = data[c].fillna(mode)\n",
    "    \n",
    "    # print(data['Yr Sold'])\n",
    "    # create useful columns from unuseful formatted data\n",
    "    data['Years Before Sale'] = data['Yr Sold'] - data['Year Built']\n",
    "    data['Years Since Remod'] = data['Yr Sold'] - data['Year Remod/Add']\n",
    "    \n",
    "    # Drop rows with negative year values\n",
    "    data = data.drop(index=[1702, 2180, 2181])\n",
    "    \n",
    "    # drop any other columns with missing values\n",
    "    missing_cnt = data.isna().sum()\n",
    "    to_drop = list(missing_cnt[missing_cnt > 0].index)\n",
    "    data = data.drop(columns = to_drop)\n",
    "    # print('cols dropped: ', to_drop)\n",
    "\n",
    "    # Drop uneeded original year columns\n",
    "    # Drop not useful columns\n",
    "    # Drop columns that leak info about the sale\n",
    "    reworked = [\"Year Built\", \"Year Remod/Add\"]\n",
    "    useless = [\"PID\", \"Order\"]\n",
    "    leaks_result = [\"Mo Sold\", \"Sale Condition\", \"Sale Type\", \"Yr Sold\"]\n",
    "    to_drop = reworked + useless + leaks_result\n",
    "    \n",
    "    data = data.drop(columns = to_drop)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55275.36731241307"
      ]
     },
     "execution_count": 450,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test the new cleaned/reworked model\n",
    "data = pd.read_csv(\"AmesHousing.tsv\", delimiter=\"\\t\")\n",
    "clean_df = transform_features(data)\n",
    "filtered_df = select_features(clean_df)\n",
    "rmse = train_and_test(filtered_df)\n",
    "\n",
    "rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BsmtFin SF 2         0.006127\n",
      "Misc Val             0.019273\n",
      "3Ssn Porch           0.032268\n",
      "Bsmt Half Bath       0.035875\n",
      "Low Qual Fin SF      0.037629\n",
      "Pool Area            0.068438\n",
      "MS SubClass          0.085128\n",
      "Overall Cond         0.101540\n",
      "Screen Porch         0.112280\n",
      "Kitchen AbvGr        0.119760\n",
      "Enclosed Porch       0.128685\n",
      "Bedroom AbvGr        0.143916\n",
      "Bsmt Unf SF          0.182751\n",
      "Lot Area             0.267520\n",
      "2nd Flr SF           0.269601\n",
      "Bsmt Full Bath       0.276258\n",
      "Half Bath            0.284871\n",
      "Open Porch SF        0.316262\n",
      "Wood Deck SF         0.328183\n",
      "BsmtFin SF 1         0.439284\n",
      "Fireplaces           0.474831\n",
      "TotRms AbvGrd        0.498574\n",
      "Mas Vnr Area         0.506983\n",
      "Years Since Remod    0.534985\n",
      "Full Bath            0.546118\n",
      "Years Before Sale    0.558979\n",
      "1st Flr SF           0.635185\n",
      "Garage Area          0.641425\n",
      "Total Bsmt SF        0.644012\n",
      "Garage Cars          0.648361\n",
      "Gr Liv Area          0.717596\n",
      "Overall Qual         0.801206\n",
      "SalePrice            1.000000\n",
      "Name: SalePrice, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "# test = np.linspace(1,5,12).reshape(4,3) # create numpy 2D array\n",
    "# find the features that correlate the most \n",
    "corr_matrix = clean_df.corr()\n",
    "hi_corr = abs(corr_matrix['SalePrice']).sort_values(ascending=True)\n",
    "print(hi_corr)\n",
    "# sns.heatmap(hi_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['BsmtFin SF 1', 'Fireplaces', 'TotRms AbvGrd', 'Mas Vnr Area', 'Years Since Remod', 'Full Bath', 'Years Before Sale', '1st Flr SF', 'Garage Area', 'Total Bsmt SF', 'Garage Cars', 'Gr Liv Area', 'Overall Qual', 'SalePrice']\n"
     ]
    }
   ],
   "source": [
    "# there seems to be a drop off in correlation at 0.4 \n",
    "# only keep columns over 0.4 correlation to the target\n",
    "print(list(hi_corr[hi_corr > 0.4].index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "low_corr = list(hi_corr[hi_corr < 0.4].index)\n",
    "clean_df = clean_df.drop(columns=low_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# list of columns from documentation *meant* to be categorical\n",
    "nominal_features = [\"PID\", \"MS SubClass\", \"MS Zoning\", \"Street\", \"Alley\", \"Land Contour\", \"Lot Config\", \"Neighborhood\", \n",
    "                    \"Condition 1\", \"Condition 2\", \"Bldg Type\", \"House Style\", \"Roof Style\", \"Roof Matl\", \"Exterior 1st\", \n",
    "                    \"Exterior 2nd\", \"Mas Vnr Type\", \"Foundation\", \"Heating\", \"Central Air\", \"Garage Type\", \n",
    "                    \"Misc Feature\", \"Sale Type\", \"Sale Condition\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 2927 entries, 0 to 2929\n",
      "Data columns (total 39 columns):\n",
      "MS Zoning            2927 non-null object\n",
      "Street               2927 non-null object\n",
      "Lot Shape            2927 non-null object\n",
      "Land Contour         2927 non-null object\n",
      "Utilities            2927 non-null object\n",
      "Lot Config           2927 non-null object\n",
      "Land Slope           2927 non-null object\n",
      "Neighborhood         2927 non-null object\n",
      "Condition 1          2927 non-null object\n",
      "Condition 2          2927 non-null object\n",
      "Bldg Type            2927 non-null object\n",
      "House Style          2927 non-null object\n",
      "Overall Qual         2927 non-null int64\n",
      "Roof Style           2927 non-null object\n",
      "Roof Matl            2927 non-null object\n",
      "Exterior 1st         2927 non-null object\n",
      "Exterior 2nd         2927 non-null object\n",
      "Mas Vnr Area         2927 non-null float64\n",
      "Exter Qual           2927 non-null object\n",
      "Exter Cond           2927 non-null object\n",
      "Foundation           2927 non-null object\n",
      "BsmtFin SF 1         2927 non-null float64\n",
      "Total Bsmt SF        2927 non-null float64\n",
      "Heating              2927 non-null object\n",
      "Heating QC           2927 non-null object\n",
      "Central Air          2927 non-null object\n",
      "1st Flr SF           2927 non-null int64\n",
      "Gr Liv Area          2927 non-null int64\n",
      "Full Bath            2927 non-null int64\n",
      "Kitchen Qual         2927 non-null object\n",
      "TotRms AbvGrd        2927 non-null int64\n",
      "Functional           2927 non-null object\n",
      "Fireplaces           2927 non-null int64\n",
      "Garage Cars          2927 non-null float64\n",
      "Garage Area          2927 non-null float64\n",
      "Paved Drive          2927 non-null object\n",
      "SalePrice            2927 non-null int64\n",
      "Years Before Sale    2927 non-null int64\n",
      "Years Since Remod    2927 non-null int64\n",
      "dtypes: float64(5), int64(9), object(25)\n",
      "memory usage: 914.7+ KB\n"
     ]
    }
   ],
   "source": [
    "clean_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# num_data = df.select_dtypes(include=['integer', 'float'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# a list of column names from documentation that are *meant* to be categorical\n",
    "nominal_features = [\"PID\", \"MS SubClass\", \"MS Zoning\", \"Street\", \"Alley\", \"Land Contour\", \"Lot Config\", \"Neighborhood\", \n",
    "                    \"Condition 1\", \"Condition 2\", \"Bldg Type\", \"House Style\", \"Roof Style\", \"Roof Matl\", \"Exterior 1st\", \n",
    "                    \"Exterior 2nd\", \"Mas Vnr Type\", \"Foundation\", \"Heating\", \"Central Air\", \"Garage Type\", \n",
    "                    \"Misc Feature\", \"Sale Type\", \"Sale Condition\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Which categorical columns have we still carried with us? We'll test these \n",
    "my_cat_intersection = list(set(clean_df.columns) & set(nominal_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## How many unique values in each categorical column?\n",
    "uniqueness_counts = clean_df[my_cat_intersection].nunique()\n",
    "\n",
    "## Aribtrary cutoff of 10 unique values (worth experimenting)\n",
    "drop_nonuniq_cols = uniqueness_counts[uniqueness_counts > 10].index\n",
    "\n",
    "clean_df = clean_df.drop(columns = drop_nonuniq_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# with the obj columns, Create dummy columns\n",
    "clean_df = pd.get_dummies(clean_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Update select_features()\n",
    "def select_features(df, coeff_threshold=0.4, uniq_threshold=10):\n",
    "    \n",
    "    corr_matrix = df.corr()['SalePrice']\n",
    "    corr_matrix = abs(corr_matrix).sort_values(ascending=True)\n",
    "    \n",
    "    # there seems to be a drop off in correlation at 0.4 \n",
    "    # only keep columns over 0.4 correlation to the target\n",
    "    # for function dymnamics, use coeff_threshold\n",
    "    low_corr = list(corr_matrix[corr_matrix < coeff_threshold].index)\n",
    "    df = df.drop(columns=low_corr)\n",
    "    \n",
    "    # list of columns from documentation *meant* to be categorical\n",
    "    nominal_features = [\"PID\", \"MS SubClass\", \"MS Zoning\", \"Street\", \"Alley\", \"Land Contour\", \"Lot Config\", \"Neighborhood\", \n",
    "                        \"Condition 1\", \"Condition 2\", \"Bldg Type\", \"House Style\", \"Roof Style\", \"Roof Matl\", \"Exterior 1st\", \n",
    "                        \"Exterior 2nd\", \"Mas Vnr Type\", \"Foundation\", \"Heating\", \"Central Air\", \"Garage Type\", \n",
    "                        \"Misc Feature\", \"Sale Type\", \"Sale Condition\"]\n",
    "    \n",
    "    ## Which categorical columns have we still carried with us? \n",
    "    cat_intersection = list(set(df.columns) & set(nominal_features))\n",
    "    \n",
    "    ## How many unique values in each categorical column?\n",
    "    uniqueness_counts = clean_df[my_cat_intersection].nunique()\n",
    "\n",
    "    ## Aribtrary cutoff of 10 unique values (worth experimenting)\n",
    "    drop_nonuniq_cols = uniqueness_counts[uniqueness_counts > 10].index\n",
    "\n",
    "    df = df.drop(columns = drop_nonuniq_cols)\n",
    "    \n",
    "    # with the obj columns, Create dummy columns, concatenate them, remove old cat columns\n",
    "    df = pd.get_dummies(df)\n",
    "    \n",
    "    # answer\n",
    "    numerical_df = df.select_dtypes(include=['int', 'float'])\n",
    "    abs_corr_coeffs = numerical_df.corr()['SalePrice'].abs().sort_values()\n",
    "    df = df.drop(abs_corr_coeffs[abs_corr_coeffs < coeff_threshold].index, axis=1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# update train_and_test\n",
    "def train_and_test(df, k=0):\n",
    "    num_data = df.select_dtypes(include=['integer', 'float'])\n",
    "    target_col = 'SalePrice'\n",
    "    feature_cols = num_data.columns.drop(target_col)\n",
    "    lr = LinearRegression()\n",
    "    \n",
    "    if k==1:\n",
    "        train = num_data.iloc[:1460, :]\n",
    "        test = num_data.iloc[1460:, :]\n",
    "\n",
    "        lr.fit(train[feature_cols], train[target_col])\n",
    "        predictions = lr.predict(test[feature_cols])\n",
    "\n",
    "        mse = mean_squared_error(test[target_col], predictions)\n",
    "        rmse = np.sqrt(mse)\n",
    "\n",
    "        return rmse\n",
    "    \n",
    "    elif k > 1:\n",
    "        kf = KFold(n_splits=k, shuffle=True, random_state=1)\n",
    "        \n",
    "        mses = cross_val_score(estimator=lr, X=df[feature_cols], y=df[target_col], scoring='neg_mean_squared_error', cv=kf)\n",
    "        rmses = np.sqrt(np.absolute(mses))\n",
    "        avg_rmse = np.mean(rmses)\n",
    "        print(rmses)\n",
    "\n",
    "        return avg_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# ANSWER\n",
    "def transform_features(df):\n",
    "    num_missing = df.isnull().sum()\n",
    "    drop_missing_cols = num_missing[(num_missing > len(df)/20)].sort_values()\n",
    "    df = df.drop(drop_missing_cols.index, axis=1)\n",
    "    \n",
    "    text_mv_counts = df.select_dtypes(include=['object']).isnull().sum().sort_values(ascending=False)\n",
    "    drop_missing_cols_2 = text_mv_counts[text_mv_counts > 0]\n",
    "    df = df.drop(drop_missing_cols_2.index, axis=1)\n",
    "    \n",
    "    num_missing = df.select_dtypes(include=['int', 'float']).isnull().sum()\n",
    "    fixable_numeric_cols = num_missing[(num_missing < len(df)/20) & (num_missing > 0)].sort_values()\n",
    "    replacement_values_dict = df[fixable_numeric_cols.index].mode().to_dict(orient='records')[0]\n",
    "    df = df.fillna(replacement_values_dict)\n",
    "    \n",
    "    years_sold = df['Yr Sold'] - df['Year Built']\n",
    "    years_since_remod = df['Yr Sold'] - df['Year Remod/Add']\n",
    "    df['Years Before Sale'] = years_sold\n",
    "    df['Years Since Remod'] = years_since_remod\n",
    "    df = df.drop([1702, 2180, 2181], axis=0)\n",
    "\n",
    "    df = df.drop([\"PID\", \"Order\", \"Mo Sold\", \"Sale Condition\", \"Sale Type\", \"Year Built\", \"Year Remod/Add\"], axis=1)\n",
    "    return df\n",
    "\n",
    "def select_features(df, coeff_threshold=0.4, uniq_threshold=10):\n",
    "    numerical_df = df.select_dtypes(include=['int', 'float'])\n",
    "    abs_corr_coeffs = numerical_df.corr()['SalePrice'].abs().sort_values()\n",
    "    df = df.drop(abs_corr_coeffs[abs_corr_coeffs < coeff_threshold].index, axis=1)\n",
    "    \n",
    "    nominal_features = [\"PID\", \"MS SubClass\", \"MS Zoning\", \"Street\", \"Alley\", \"Land Contour\", \"Lot Config\", \"Neighborhood\", \n",
    "                    \"Condition 1\", \"Condition 2\", \"Bldg Type\", \"House Style\", \"Roof Style\", \"Roof Matl\", \"Exterior 1st\", \n",
    "                    \"Exterior 2nd\", \"Mas Vnr Type\", \"Foundation\", \"Heating\", \"Central Air\", \"Garage Type\", \n",
    "                    \"Misc Feature\", \"Sale Type\", \"Sale Condition\"]\n",
    "    \n",
    "    transform_cat_cols = []\n",
    "    for col in nominal_features:\n",
    "        if col in df.columns:\n",
    "            transform_cat_cols.append(col)\n",
    "\n",
    "    uniqueness_counts = df[transform_cat_cols].apply(lambda col: len(col.value_counts())).sort_values()\n",
    "    drop_nonuniq_cols = uniqueness_counts[uniqueness_counts > 10].index\n",
    "    df = df.drop(drop_nonuniq_cols, axis=1)\n",
    "    \n",
    "    text_cols = df.select_dtypes(include=['object'])\n",
    "    for col in text_cols:\n",
    "        df[col] = df[col].astype('category')\n",
    "    df = pd.concat([df, pd.get_dummies(df.select_dtypes(include=['category']))], axis=1).drop(text_cols,axis=1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ANSWER\n",
    "def train_and_test2(df, k=0):\n",
    "    numeric_df = df.select_dtypes(include=['integer', 'float'])\n",
    "    features = numeric_df.columns.drop(\"SalePrice\")\n",
    "    lr = LinearRegression()\n",
    "    \n",
    "    if k == 0:\n",
    "        train = df[:1460]\n",
    "        test = df[1460:]\n",
    "\n",
    "        lr.fit(train[features], train[\"SalePrice\"])\n",
    "        predictions = lr.predict(test[features])\n",
    "        mse = mean_squared_error(test[\"SalePrice\"], predictions)\n",
    "        rmse = np.sqrt(mse)\n",
    "\n",
    "        return rmse\n",
    "    \n",
    "    if k == 1:\n",
    "        # Randomize *all* rows (frac=1) from `df` and return\n",
    "        shuffled_df = df.sample(frac=1, )\n",
    "        train = df[:1460]\n",
    "        test = df[1460:]\n",
    "        \n",
    "        lr.fit(train[features], train[\"SalePrice\"])\n",
    "        predictions_one = lr.predict(test[features])        \n",
    "        \n",
    "        mse_one = mean_squared_error(test[\"SalePrice\"], predictions_one)\n",
    "        rmse_one = np.sqrt(mse_one)\n",
    "        \n",
    "        lr.fit(test[features], test[\"SalePrice\"])\n",
    "        predictions_two = lr.predict(train[features])        \n",
    "       \n",
    "        mse_two = mean_squared_error(train[\"SalePrice\"], predictions_two)\n",
    "        rmse_two = np.sqrt(mse_two)\n",
    "        \n",
    "        avg_rmse = np.mean([rmse_one, rmse_two])\n",
    "        print(rmse_one)\n",
    "        print(rmse_two)\n",
    "        return avg_rmse\n",
    "    else:\n",
    "        kf = KFold(n_splits=k, shuffle=True)\n",
    "        rmse_values = []\n",
    "        for train_index, test_index, in kf.split(df):\n",
    "            train = df.iloc[train_index]\n",
    "            test = df.iloc[test_index]\n",
    "            lr.fit(train[features], train[\"SalePrice\"])\n",
    "            predictions = lr.predict(test[features])\n",
    "            mse = mean_squared_error(test[\"SalePrice\"], predictions)\n",
    "            rmse = np.sqrt(mse)\n",
    "            rmse_values.append(rmse)\n",
    "        print(rmse_values)\n",
    "        avg_rmse = np.mean(rmse_values)\n",
    "        return avg_rmse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[26885.064689311057, 26062.376114660387, 35262.29975501161, 27788.565675199163]\n",
      "28999.576558545556\n",
      "[36756.52485284 25652.0636658  25571.38314607 28465.76822678]\n",
      "29111.4349728706\n",
      "[25444.793633697584, 28452.993070891287, 35372.62847431943, 26316.814758464134]\n",
      "28896.80748434311\n",
      "[36756.52485284 25652.0636658  25571.38314607 28465.76822678]\n",
      "29111.4349728706\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"AmesHousing.tsv\", delimiter=\"\\t\")\n",
    "tf = transform_features(df)\n",
    "filtered_df = select_features(tf)\n",
    "\n",
    "rmse = train_and_test2(filtered_df, k=4)\n",
    "print(rmse)\n",
    "rmse = train_and_test(filtered_df, k=4)\n",
    "print(rmse)\n",
    "\n",
    "rmse = train_and_test2(clean_df, k=4)\n",
    "print(rmse)\n",
    "rmse = train_and_test(clean_df, k=4)\n",
    "print(rmse)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
